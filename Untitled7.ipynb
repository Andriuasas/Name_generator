{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNiJfTc0Up2iyKyYnX403jz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andriuasas/Name_generator/blob/main/Untitled7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "ziyCwDYfbbZ1"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pradedame nuo duomenų scrape'inimo"
      ],
      "metadata": {
        "id": "ii7jBktHf-Qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to scrape and save names for both male and female categories\n",
        "def scrape_and_save_names():\n",
        "    male_names = []\n",
        "    female_names = []\n",
        "\n",
        "    # Scrape male names\n",
        "    for key in ['a', 'b', 'c', 'c-2', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "                'm', 'n', 'o', 'p', 'r', 's', 's-2', 't', 'u', 'v', 'z', 'z-2']:\n",
        "        url = f'https://vardai.vlkk.lt/sarasas/{key}/'\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        links = soup.find_all('a', class_='names_list__links names_list__links--man')\n",
        "        male_names += [name.text.lower() for name in links]  # Convert names to lowercase\n",
        "\n",
        "    # Scrape female names\n",
        "    for key in ['a', 'b', 'c', 'c-2', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "                'm', 'n', 'o', 'p', 'r', 's', 's-2', 't', 'u', 'v', 'z', 'z-2']:\n",
        "        url = f'https://vardai.vlkk.lt/sarasas/{key}/'\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        links = soup.find_all('a', class_='names_list__links names_list__links--woman')\n",
        "        female_names += [name.text.lower() for name in links]  # Convert names to lowercase\n",
        "\n",
        "    # Save male and female names to their respective text files\n",
        "    male_names = [f'-{name}' for name in male_names]  # Prefix male names with '-'\n",
        "    female_names = [f'_{name}' for name in female_names]  # Prefix female names with '_'\n",
        "\n",
        "    np.savetxt('Mvardai.txt', male_names, fmt='%s', header='name', comments='', newline='\\n')\n",
        "    np.savetxt('Fvardai.txt', female_names, fmt='%s', header='name', comments='', newline='\\n')\n",
        "\n",
        "# Call the function to scrape and save names\n",
        "scrape_and_save_names()"
      ],
      "metadata": {
        "id": "7iV03ytzbmvi"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Konvertuojame galimas raides į skaičius ir atvirkščiai. Didžiosios, pirmosios vardų raidės tampa mažosiomis."
      ],
      "metadata": {
        "id": "oO30idhRgRJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataset class for male and female names\n",
        "class NameDataset(Dataset):\n",
        "    def __init__(self, *csv_files):\n",
        "        self.names = []\n",
        "        self.labels = []  # Labels for gender (0 for male, 1 for female)\n",
        "\n",
        "        # Load names from both male and female text files\n",
        "        for file in csv_files:\n",
        "            data = np.loadtxt(file, dtype=str, skiprows=1)  # Skip header\n",
        "            for name in data:\n",
        "                self.names.append(name[1:])  # Exclude gender prefix for model training\n",
        "                self.labels.append(0 if name[0] == '-' else 1)  # Label: 0 for male, 1 for female\n",
        "\n",
        "        self.chars = sorted(list(set(''.join(self.names) + ' ')))  # Including padding character\n",
        "        self.char_to_int = {c: i for i, c in enumerate(self.chars)}\n",
        "        self.int_to_char = {i: c for c, i in self.char_to_int.items()}\n",
        "        self.vocab_size = len(self.chars)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.names[idx] + ' '  # Adding padding character at the end\n",
        "        encoded_name = [self.char_to_int[char] for char in name]\n",
        "        label = self.labels[idx]\n",
        "        return torch.tensor(encoded_name), torch.tensor(label)\n"
      ],
      "metadata": {
        "id": "6TXqyM1hbzhb"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Atliekame padding'ą. Taip pat išskiriame vyriškus vardus su '-', bei moteriškus su '_', atitinkamai.\n"
      ],
      "metadata": {
        "id": "Zwch8SSSgjN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom collate function for padding sequences\n",
        "def pad_collate(batch):\n",
        "    padded_seqs = pad_sequence([item[0] for item in batch], batch_first=True, padding_value=0)\n",
        "    input_seq = padded_seqs[:, :-1]\n",
        "    target_seq = padded_seqs[:, 1:]\n",
        "    labels = torch.tensor([item[1] for item in batch])  # Gender labels for each name\n",
        "    return input_seq, target_seq, labels\n",
        "\n",
        "# Load dataset and create dataloader\n",
        "dataset = NameDataset('Mvardai.txt', 'Fvardai.txt')\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=pad_collate)"
      ],
      "metadata": {
        "id": "YlTSJyB4gibE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pasitikrinimui."
      ],
      "metadata": {
        "id": "kWemkjtyhbjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(dataloader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6-643kzhVBz",
        "outputId": "d5a3f0df-9d35-43d2-a9ef-7bbaa4da33bb"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[17,  1, 49, 14,  9, 10,  1,  0,  0,  0],\n",
              "         [ 6, 17,  1, 18,  5, 14,  9, 10,  1,  0],\n",
              "         [10, 20, 21,  1, 47,  0,  0,  0,  0,  0],\n",
              "         [21,  9, 11, 19,  1, 17,  1, 18,  0,  0],\n",
              "         [21,  9, 47, 12, 14,  9, 20, 18,  0,  0],\n",
              "         [16, 17,  9, 24,  1,  0,  0,  0,  0,  0],\n",
              "         [10,  5,  7,  5, 14,  9, 10, 20, 18,  0],\n",
              "         [14, 15, 48, 17, 19,  1, 20, 19, 40,  0],\n",
              "         [ 9, 25,  1,  2,  5, 47, 12, 40,  0,  0],\n",
              "         [ 1, 12, 25,  2,  9,  5, 19,  1,  0,  0],\n",
              "         [18,  1, 21,  9, 14,  1, 47,  0,  0,  0],\n",
              "         [19,  5, 15,  4, 15, 47, 17,  1, 18,  0],\n",
              "         [35, 17,  9, 20, 18,  0,  0,  0,  0,  0],\n",
              "         [21,  9, 47, 12,  7,  1, 17,  4,  1, 18],\n",
              "         [ 1, 20, 49, 21, 24,  4,  1, 18,  0,  0],\n",
              "         [17,  5, 14,  1, 19, 18,  0,  0,  0,  0],\n",
              "         [11,  5, 47, 19, 40,  0,  0,  0,  0,  0],\n",
              "         [16, 17,  1, 11, 18, 40,  4,  1,  0,  0],\n",
              "         [ 4,  1, 20, 49,  7,  1, 17,  4,  1, 18],\n",
              "         [ 1,  9, 49, 14,  9, 20, 18,  0,  0,  0],\n",
              "         [ 2,  5,  1, 19, 17,  9, 18,  1, 47,  0],\n",
              "         [ 1,  2,  9,  7, 27,  9, 12, 40,  0,  0],\n",
              "         [ 9, 10,  1, 47,  0,  0,  0,  0,  0,  0],\n",
              "         [ 7,  5,  4,  5, 49, 14, 40,  0,  0,  0],\n",
              "         [ 4,  9, 12,  1, 17,  1, 47,  0,  0,  0],\n",
              "         [ 7, 17,  9, 11,  5, 17,  9, 10,  1,  0],\n",
              "         [18, 14,  1,  9, 49,  7, 40,  0,  0,  0],\n",
              "         [18,  1, 14,  1, 47,  0,  0,  0,  0,  0],\n",
              "         [ 1, 20, 11, 18, 24, 49, 18,  0,  0,  0],\n",
              "         [ 7,  5, 17, 21,  9, 47, 12, 40,  0,  0],\n",
              "         [13,  9, 14, 26,  0,  0,  0,  0,  0,  0],\n",
              "         [10,  1, 21,  9, 19,  1, 18,  0,  0,  0]]),\n",
              " tensor([[ 1, 49, 14,  9, 10,  1,  0,  0,  0,  0],\n",
              "         [17,  1, 18,  5, 14,  9, 10,  1,  0,  0],\n",
              "         [20, 21,  1, 47,  0,  0,  0,  0,  0,  0],\n",
              "         [ 9, 11, 19,  1, 17,  1, 18,  0,  0,  0],\n",
              "         [ 9, 47, 12, 14,  9, 20, 18,  0,  0,  0],\n",
              "         [17,  9, 24,  1,  0,  0,  0,  0,  0,  0],\n",
              "         [ 5,  7,  5, 14,  9, 10, 20, 18,  0,  0],\n",
              "         [15, 48, 17, 19,  1, 20, 19, 40,  0,  0],\n",
              "         [25,  1,  2,  5, 47, 12, 40,  0,  0,  0],\n",
              "         [12, 25,  2,  9,  5, 19,  1,  0,  0,  0],\n",
              "         [ 1, 21,  9, 14,  1, 47,  0,  0,  0,  0],\n",
              "         [ 5, 15,  4, 15, 47, 17,  1, 18,  0,  0],\n",
              "         [17,  9, 20, 18,  0,  0,  0,  0,  0,  0],\n",
              "         [ 9, 47, 12,  7,  1, 17,  4,  1, 18,  0],\n",
              "         [20, 49, 21, 24,  4,  1, 18,  0,  0,  0],\n",
              "         [ 5, 14,  1, 19, 18,  0,  0,  0,  0,  0],\n",
              "         [ 5, 47, 19, 40,  0,  0,  0,  0,  0,  0],\n",
              "         [17,  1, 11, 18, 40,  4,  1,  0,  0,  0],\n",
              "         [ 1, 20, 49,  7,  1, 17,  4,  1, 18,  0],\n",
              "         [ 9, 49, 14,  9, 20, 18,  0,  0,  0,  0],\n",
              "         [ 5,  1, 19, 17,  9, 18,  1, 47,  0,  0],\n",
              "         [ 2,  9,  7, 27,  9, 12, 40,  0,  0,  0],\n",
              "         [10,  1, 47,  0,  0,  0,  0,  0,  0,  0],\n",
              "         [ 5,  4,  5, 49, 14, 40,  0,  0,  0,  0],\n",
              "         [ 9, 12,  1, 17,  1, 47,  0,  0,  0,  0],\n",
              "         [17,  9, 11,  5, 17,  9, 10,  1,  0,  0],\n",
              "         [14,  1,  9, 49,  7, 40,  0,  0,  0,  0],\n",
              "         [ 1, 14,  1, 47,  0,  0,  0,  0,  0,  0],\n",
              "         [20, 11, 18, 24, 49, 18,  0,  0,  0,  0],\n",
              "         [ 5, 17, 21,  9, 47, 12, 40,  0,  0,  0],\n",
              "         [ 9, 14, 26,  0,  0,  0,  0,  0,  0,  0],\n",
              "         [ 1, 21,  9, 19,  1, 18,  0,  0,  0,  0]]),\n",
              " tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 0, 1, 1, 0]))"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apsirašome modelį."
      ],
      "metadata": {
        "id": "O96ybJSUhJKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Minimal Transformer Model\n",
        "class MinimalTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, forward_expansion):\n",
        "        super(MinimalTransformer, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, 100, embed_size))\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
        "        self.output_layer = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        positions = torch.arange(0, x.size(1)).unsqueeze(0)\n",
        "        x = self.embed(x) + self.positional_encoding[:, :x.size(1), :]\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "-A61_nvwhJ7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treniruojame modelį."
      ],
      "metadata": {
        "id": "LBMYAo0Mg5V6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train_model(model, dataloader, epochs=10):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Ensure the model is in training mode\n",
        "        total_loss = 0.0\n",
        "        batch_count = 0\n",
        "\n",
        "        for batch_idx, (input_seq, target_seq, labels) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            output = model(input_seq)\n",
        "            loss = criterion(output.transpose(1, 2), target_seq)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            batch_count += 1\n",
        "\n",
        "        average_loss = total_loss / batch_count\n",
        "        print(f'Epoch {epoch+1}, Average Loss: {average_loss}')\n",
        "\n",
        "# Initialize and train the model\n",
        "model = MinimalTransformer(vocab_size=dataset.vocab_size, embed_size=128, num_heads=8, forward_expansion=4)\n",
        "train_model(model, dataloader)\n",
        "\n",
        "# Sampling function to generate names based on the trained model\n",
        "def sample(model, dataset, start_str='a', max_length=20):\n",
        "    model.eval()  # Switch to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        # Convert start string to tensor\n",
        "        chars = [dataset.char_to_int[c] for c in start_str]\n",
        "        input_seq = torch.tensor(chars).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        output_name = start_str\n",
        "        for _ in range(max_length - len(start_str)):\n",
        "            output = model(input_seq)\n",
        "\n",
        "            # Get the last character from the output\n",
        "            probabilities = torch.softmax(output[0, -1], dim=0)\n",
        "            # Sample a character from the probability distribution\n",
        "            next_char_idx = torch.multinomial(probabilities, 1).item()\n",
        "            next_char = dataset.int_to_char[next_char_idx]\n",
        "\n",
        "            if next_char == ' ':  # Assume ' ' is your end-of-sequence character\n",
        "                break\n",
        "\n",
        "            output_name += next_char\n",
        "            # Update the input sequence for the next iteration\n",
        "            input_seq = torch.cat([input_seq, torch.tensor([[next_char_idx]])], dim=1)\n",
        "\n",
        "        return output_name.capitalize()  # Capitalize the first letter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7bAYXD6b0I4",
        "outputId": "2506c248-5c81-415b-fee3-3b1647a7c883"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Loss: 1.5140078543674333\n",
            "Epoch 2, Average Loss: 1.3867184008534246\n",
            "Epoch 3, Average Loss: 1.3671126803861777\n",
            "Epoch 4, Average Loss: 1.3640464577279072\n",
            "Epoch 5, Average Loss: 1.3524489812700173\n",
            "Epoch 6, Average Loss: 1.3547129993853362\n",
            "Epoch 7, Average Loss: 1.3473644511030596\n",
            "Epoch 8, Average Loss: 1.3404924106220955\n",
            "Epoch 9, Average Loss: 1.343177014659987\n",
            "Epoch 10, Average Loss: 1.3415513274226736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pasižiūrėkime, kaip sekėsi."
      ],
      "metadata": {
        "id": "BB1w50PIhAjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After training your model, generate names\n",
        "for _ in range(10):\n",
        "    generated_name = sample(model, dataset, start_str='l')\n",
        "    print(generated_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjtcyaM5dDYo",
        "outputId": "a076b9df-9a2c-4135-d8c1-71af4bf89f10"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lìmìtė\n",
            "Lĩjuzas\n",
            "Licientìlinasìlalo\n",
            "Liginanas\n",
            "Lorviltas\n",
            "Lìlòlijus\n",
            "Lirlentas\n",
            "Lãmija\n",
            "Latãtãras\n",
            "Lilèmatas\n"
          ]
        }
      ]
    }
  ]
}