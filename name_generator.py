# -*- coding: utf-8 -*-
"""Name_generator

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T496CGO7zOXa4Klbp--Np9fKR3EZR9fX
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, Dataset
from torch.nn.utils.rnn import pad_sequence

import requests
from bs4 import BeautifulSoup

"""Pradedame nuo duomenų scrape'inimo"""

# Function to scrape and save names for both male and female categories
def scrape_and_save_names():
    male_names = []
    female_names = []

    # Scrape male names
    for key in ['a', 'b', 'c', 'c-2', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',
                'm', 'n', 'o', 'p', 'r', 's', 's-2', 't', 'u', 'v', 'z', 'z-2']:
        url = f'https://vardai.vlkk.lt/sarasas/{key}/'
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        links = soup.find_all('a', class_='names_list__links names_list__links--man')
        male_names += [name.text.lower() for name in links]  # Convert names to lowercase

    # Scrape female names
    for key in ['a', 'b', 'c', 'c-2', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',
                'm', 'n', 'o', 'p', 'r', 's', 's-2', 't', 'u', 'v', 'z', 'z-2']:
        url = f'https://vardai.vlkk.lt/sarasas/{key}/'
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        links = soup.find_all('a', class_='names_list__links names_list__links--woman')
        female_names += [name.text.lower() for name in links]  # Convert names to lowercase

    # Save male and female names to their respective text files
    male_names = [f'-{name}' for name in male_names]  # Prefix male names with '-'
    female_names = [f'_{name}' for name in female_names]  # Prefix female names with '_'

    np.savetxt('Mvardai.txt', male_names, fmt='%s', header='name', comments='', newline='\n')
    np.savetxt('Fvardai.txt', female_names, fmt='%s', header='name', comments='', newline='\n')

# Call the function to scrape and save names
scrape_and_save_names()

"""Konvertuojame galimas raides į skaičius ir atvirkščiai. Didžiosios, pirmosios vardų raidės tampa mažosiomis."""

# Create a dataset class for male and female names
class NameDataset(Dataset):
    def __init__(self, *csv_files):
        self.names = []
        self.labels = []  # Labels for gender (0 for male, 1 for female)

        # Load names from both male and female text files
        for file in csv_files:
            data = np.loadtxt(file, dtype=str, skiprows=1)  # Skip header
            for name in data:
                self.names.append(name[1:])  # Exclude gender prefix for model training
                self.labels.append(0 if name[0] == '-' else 1)  # Label: 0 for male, 1 for female

        self.chars = sorted(list(set(''.join(self.names) + ' ')))  # Including padding character
        self.char_to_int = {c: i for i, c in enumerate(self.chars)}
        self.int_to_char = {i: c for c, i in self.char_to_int.items()}
        self.vocab_size = len(self.chars)

    def __len__(self):
        return len(self.names)

    def __getitem__(self, idx):
        name = self.names[idx] + ' '  # Adding padding character at the end
        encoded_name = [self.char_to_int[char] for char in name]
        label = self.labels[idx]
        return torch.tensor(encoded_name), torch.tensor(label)

"""Atliekame padding'ą. Taip pat išskiriame vyriškus vardus su '-', bei moteriškus su '_', atitinkamai.

"""

# Custom collate function for padding sequences
def pad_collate(batch):
    padded_seqs = pad_sequence([item[0] for item in batch], batch_first=True, padding_value=0)
    input_seq = padded_seqs[:, :-1]
    target_seq = padded_seqs[:, 1:]
    labels = torch.tensor([item[1] for item in batch])  # Gender labels for each name
    return input_seq, target_seq, labels

# Load dataset and create dataloader
dataset = NameDataset('Mvardai.txt', 'Fvardai.txt')
dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=pad_collate)

"""Pasitikrinimui."""

next(iter(dataloader))

"""Apsirašome modelį."""

# Minimal Transformer Model
class MinimalTransformer(nn.Module):
    def __init__(self, vocab_size, embed_size, num_heads, forward_expansion):
        super(MinimalTransformer, self).__init__()
        self.embed = nn.Embedding(vocab_size, embed_size)
        self.positional_encoding = nn.Parameter(torch.randn(1, 100, embed_size))
        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads)
        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)
        self.output_layer = nn.Linear(embed_size, vocab_size)

    def forward(self, x):
        positions = torch.arange(0, x.size(1)).unsqueeze(0)
        x = self.embed(x) + self.positional_encoding[:, :x.size(1), :]
        x = self.transformer_encoder(x)
        x = self.output_layer(x)
        return x

"""Treniruojame modelį."""

# Training loop
def train_model(model, dataloader, epochs=10):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters())

    for epoch in range(epochs):
        model.train()  # Ensure the model is in training mode
        total_loss = 0.0
        batch_count = 0

        for batch_idx, (input_seq, target_seq, labels) in enumerate(dataloader):
            optimizer.zero_grad()
            output = model(input_seq)
            loss = criterion(output.transpose(1, 2), target_seq)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            batch_count += 1

        average_loss = total_loss / batch_count
        print(f'Epoch {epoch+1}, Average Loss: {average_loss}')

# Initialize and train the model
model = MinimalTransformer(vocab_size=dataset.vocab_size, embed_size=128, num_heads=8, forward_expansion=4)
train_model(model, dataloader)

# Sampling function to generate names based on the trained model
def sample(model, dataset, start_str='a', max_length=20):
    model.eval()  # Switch to evaluation mode
    with torch.no_grad():
        # Convert start string to tensor
        chars = [dataset.char_to_int[c] for c in start_str]
        input_seq = torch.tensor(chars).unsqueeze(0)  # Add batch dimension

        output_name = start_str
        for _ in range(max_length - len(start_str)):
            output = model(input_seq)

            # Get the last character from the output
            probabilities = torch.softmax(output[0, -1], dim=0)
            # Sample a character from the probability distribution
            next_char_idx = torch.multinomial(probabilities, 1).item()
            next_char = dataset.int_to_char[next_char_idx]

            if next_char == ' ':  # Assume ' ' is your end-of-sequence character
                break

            output_name += next_char
            # Update the input sequence for the next iteration
            input_seq = torch.cat([input_seq, torch.tensor([[next_char_idx]])], dim=1)

        return output_name.capitalize()  # Capitalize the first letter

"""Pasižiūrėkime, kaip sekėsi."""

# After training your model, generate names
for _ in range(10):
    generated_name = sample(model, dataset, start_str='l')
    print(generated_name)